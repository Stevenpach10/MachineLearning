{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "from os.path import isdir\n",
    "from os.path import join\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import shutil\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misc functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  X = []\n",
    "  Y = []\n",
    "\n",
    "  onlyDirs = [f for f in listdir(path) if isdir(join(path, f))]\n",
    "\n",
    "  for category in onlyDirs:\n",
    "    pos_y = onlyDirs.index(category)\n",
    "    print(category, pos_y)\n",
    "    tempDirInput = join(path, category)\n",
    "    for f in listdir(tempDirInput):\n",
    "      inputPath = (join(tempDirInput, f))\n",
    "      if isfile(inputPath):\n",
    "        X.append(np.load(inputPath))\n",
    "        one_hot = np.zeros((len(onlyDirs)))\n",
    "        one_hot[pos_y] = 1\n",
    "        Y.append(one_hot)\n",
    "\n",
    "  return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    \n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    \n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About metrics.\n",
    "# Metric dictionary keys \n",
    "_loss            = 'Loss'\n",
    "_accuracy        = 'Accuracy'\n",
    "_accuracyClass   = 'Accuracy class'\n",
    "_confusionMatrix = 'Confusion matrix'\n",
    "_groundtruth     = 'Groundtruth'\n",
    "_predictions     = 'Predictions'\n",
    "# Get a clean dictionary for the metrics.\n",
    "def getMetricsDict(config):\n",
    "    return {\n",
    "        _loss            : torch.tensor(0.),\n",
    "        _accuracy        : torch.tensor(0.),\n",
    "        _accuracyClass   : torch.zeros(config[classesLen]),\n",
    "        _confusionMatrix : torch.zeros((config[classesLen], config[classesLen]), dtype=torch.int),\n",
    "        _groundtruth     : torch.tensor([]),\n",
    "        _predictions     : torch.tensor([])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print the metrics dictionaries.\n",
    "def printMetricsDict(metricsResults, config):\n",
    "    # Build accuracy by class.\n",
    "    accuracyClassStr = ''\n",
    "    for i, _class in enumerate(config[classes]):\n",
    "        accuracyClassStr += '{}: {:2.2f}%'.format(_class, metricsResults[_accuracyClass][i] * 100)\n",
    "        accuracyClassStr += ', '\n",
    "    accuracyClassStr = accuracyClassStr[:-2]\n",
    "\n",
    "    print('Loss: {:.4f}, Accuracy: {:2.2f}% ({})'.format(metricsResults[_loss], metricsResults[_accuracy] * 100, accuracyClassStr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to update the dictionary of resulting metrics.\n",
    "def updateRunningMetrics(outputs, groundtruth, loss, batchAmount, metricsResults, config):\n",
    "    # Accumulate the loss.\n",
    "    metricsResults[_loss] += loss.cpu() / batchAmount\n",
    "    # Accumulate the confusion matrix.\n",
    "    confusionMatrix = getConfusionMatrix(outputs, groundtruth, config)\n",
    "    metricsResults[_confusionMatrix] += confusionMatrix\n",
    "    metricsResults[_groundtruth] = torch.cat((metricsResults[_groundtruth], groundtruth)) \n",
    "    metricsResults[_predictions] = torch.cat((metricsResults[_predictions], outputs))\n",
    "\n",
    "# Function used to process the dictionary of resulting metrics (make final calculations).\n",
    "def processRunningMetrics(metricsResults):\n",
    "    # Get the total of samples processed by class.\n",
    "    classTotal = torch.sum(metricsResults[_confusionMatrix], 1)\n",
    "    # Get the total of samples correctly classified by class.\n",
    "    classCorrect = torch.diagonal(metricsResults[_confusionMatrix])\n",
    "\n",
    "    # Get the total accuracy, correct total samples / total samples.\n",
    "    metricsResults[_accuracy] = torch.sum(classCorrect) / torch.sum(classTotal)\n",
    "    # Get the total accuracy, by class.\n",
    "    metricsResults[_accuracyClass] = classCorrect / classTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions process an metrics result dictionary for wandb. Is necessary to indicte\n",
    "#   the metrics origin, training or testing.\n",
    "def processMetricsWandb(metricsResults, config, training=False):\n",
    "    # Get the prefix to log on wandb, the keys must be different.\n",
    "    resultsType = 'training' if training else 'testing'\n",
    "\n",
    "    # All the wandb keys are based in the original metrics results keys.\n",
    "    lossKey = '{} ({})'.format(_loss, resultsType)\n",
    "    accuracyKey = '{} ({})'.format(_accuracy, resultsType)\n",
    "    accuracyClassKeys = ['{} accuracy ({})'.format(_class, resultsType) for _class in config[classes]]\n",
    "\n",
    "    confusionMatrixKey = '{} ({})'.format(_confusionMatrix, resultsType)\n",
    "    y_t = torch.argmax(metricsResults[_groundtruth],axis=1)\n",
    "    y_p = torch.argmax(metricsResults[_predictions], axis=1)\n",
    "    heatMap = wandb.plot.confusion_matrix(y_true=y_t.tolist(), preds=y_p.tolist(), class_names=config[classes], title=confusionMatrixKey)\n",
    "\n",
    "    # Make the dictionary for wandb and store the values.\n",
    "    wandbDict = {\n",
    "        lossKey             : metricsResults[_loss].item(),\n",
    "        accuracyKey         : metricsResults[_accuracy].item(),\n",
    "        confusionMatrixKey  : heatMap\n",
    "    }\n",
    "    for i in range(config[classesLen]):\n",
    "        wandbDict[accuracyClassKeys[i]] = metricsResults[_accuracyClass][i].item()\n",
    "\n",
    "    # Return, to log later.\n",
    "    return wandbDict\n",
    "    \n",
    "# Get the metrics dictionaries for wandb and log them.\n",
    "def logMetricsWandb(trainMetricsResults, testMetricsResults, config):\n",
    "    # Get both dictionaries for wandb.\n",
    "    wandbTrainDict = processMetricsWandb(trainMetricsResults, config, training=True)\n",
    "    wandbTestDict  = processMetricsWandb(testMetricsResults, config, training=False)\n",
    "\n",
    "    # Merge the dictionaries.\n",
    "    wandbDict = {**wandbTrainDict, **wandbTestDict}\n",
    "\n",
    "    # Log on wandb\n",
    "    wandb.log(wandbDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the confusion matrix values.\n",
    "def getConfusionMatrix(outputs, groundtruth, config):\n",
    "    # Init the confusion matrix.\n",
    "    confusionMatrix = torch.zeros((config[classesLen], config[classesLen]), dtype=torch.int)\n",
    "\n",
    "    # Obtain the predictions (the greater number, because we use a one hot vector).\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Iterate the predictions.\n",
    "    for i in range(predicted.shape[0]):\n",
    "        # Add 1 based on the prediction done for a specific label.\n",
    "        \n",
    "        confusionMatrix[torch.argmax(groundtruth[i])][predicted[i]] += 1\n",
    "\n",
    "    return confusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths\n",
    "histOutputDir = 'data/pp/hist'\n",
    "\n",
    "run_ID = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = load_data(os.path.join(histOutputDir,'test'))\n",
    "X_train, y_train = load_data(os.path.join(histOutputDir,'train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input size of the model.\n",
    "inputSize = 'inputSize'\n",
    "# Output size of the model.\n",
    "outputSize = 'outputSize'\n",
    "# Batch size.\n",
    "batchSize = 'batchSize'\n",
    "# Epochs amount.\n",
    "epochs = 'epochs'\n",
    "# Learning rate.\n",
    "learningRate = 'learningRate'\n",
    "#FC_Layer\n",
    "fc_size_1 = 'fc_size_1'\n",
    "fc_size_2 = 'fc_size_2'\n",
    "fc_size_3 = 'fc_size_3'\n",
    "fc_size_4 = 'fc_size_4'\n",
    "fc_size_5 = 'fc_size_5'\n",
    "fc_size_6 = 'fc_size_6'\n",
    "# Class names.\n",
    "classes = 'classes'\n",
    "# Number of classes to classify.\n",
    "classesLen = 'classesLen'\n",
    "name = 'name'\n",
    "\n",
    "upSampling = 'upSampling'\n",
    "\n",
    "exp_1 = {\n",
    "    epochs        :   21,\n",
    "    learningRate :   0.0001,\n",
    "    batchSize    :   32,\n",
    "    inputSize     :   256,\n",
    "    fc_size_1     :   256,\n",
    "    fc_size_2     :   128,\n",
    "    fc_size_3     :   64,\n",
    "    fc_size_4     :   32,\n",
    "    fc_size_5     :   16,\n",
    "    fc_size_6     :   8,\n",
    "    outputSize      :   4,\n",
    "    classes      : ['COVID', 'Lung Opacity', 'Normal', 'Viral Pneumonia'],\n",
    "    classesLen     : 4,\n",
    "    upSampling     : False,\n",
    "    name           : 'WithoutUpsampling_32Batch'\n",
    "}\n",
    "exp_2 = {\n",
    "    epochs        :   21,\n",
    "    learningRate :   0.0001,\n",
    "    batchSize    :   32,\n",
    "    inputSize     :   256,\n",
    "    fc_size_1     :   256,\n",
    "    fc_size_2     :   128,\n",
    "    fc_size_3     :   64,\n",
    "    fc_size_4     :   32,\n",
    "    fc_size_5     :   16,\n",
    "    fc_size_6     :   8,\n",
    "    outputSize      :   4,\n",
    "    classes      : ['COVID', 'Lung Opacity', 'Normal', 'Viral Pneumonia'],\n",
    "    classesLen     : 4,\n",
    "    upSampling     : True,\n",
    "    name           : 'WithUpsampling_32Batch'\n",
    "}\n",
    "exp_3 = {\n",
    "    epochs        :   21,\n",
    "    learningRate :   0.0001,\n",
    "    batchSize    :   16,\n",
    "    inputSize     :   256,\n",
    "    fc_size_1     :   256,\n",
    "    fc_size_2     :   128,\n",
    "    fc_size_3     :   64,\n",
    "    fc_size_4     :   32,\n",
    "    fc_size_5     :   16,\n",
    "    fc_size_6     :   8,\n",
    "    outputSize      :   4,\n",
    "    classes      : ['COVID', 'Lung Opacity', 'Normal', 'Viral Pneumonia'],\n",
    "    classesLen     : 4,\n",
    "    upSampling     : False,\n",
    "    name           : 'WithoutUpsampling_16Batch'\n",
    "}\n",
    "exp_4 = {\n",
    "    epochs        :   21,\n",
    "    learningRate :   0.0001,\n",
    "    batchSize    :   16,\n",
    "    inputSize     :   256,\n",
    "    fc_size_1     :   256,\n",
    "    fc_size_2     :   128,\n",
    "    fc_size_3     :   64,\n",
    "    fc_size_4     :   32,\n",
    "    fc_size_5     :   16,\n",
    "    fc_size_6     :   8,\n",
    "    outputSize      :   4,\n",
    "    classes      : ['COVID', 'Lung Opacity', 'Normal', 'Viral Pneumonia'],\n",
    "    classesLen     : 4,\n",
    "    upSampling     : True,\n",
    "    name           : 'WithUpsampling_16Batch'\n",
    "}\n",
    "\n",
    "configs = [exp_1, exp_2, exp_3, exp_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UpSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upSamplingData(config):\n",
    "    if config[upSampling]:\n",
    "        dic_values = {}\n",
    "        weight = {}\n",
    "        for class_type in np.unique(y_train, axis=0):\n",
    "            dic_values[int(np.argmax(class_type))] = len(np.where((y_train == class_type).all(axis=1))[0])\n",
    "            weight[int(np.argmax(class_type))] = float(1.0/len(np.unique(y_train, axis=0)))/dic_values[int (np.argmax(class_type))]\n",
    "\n",
    "        print(\"dic_values\", dic_values)\n",
    "        print(\"weight\", weight)\n",
    "\n",
    "        samples_weight = np.array([weight[int(np.argmax(t))] for t in y_train])\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "        return sampler\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampler option is mutually exclusive with shuffle option.\n",
    "def setShuffle(config):\n",
    "    if config[upSampling]:\n",
    "        return False\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataLoaders(config):\n",
    "        tensor_x = torch.Tensor(X_train) \n",
    "        tensor_y = torch.Tensor(y_train)\n",
    "        my_dataset = TensorDataset(tensor_x,tensor_y) \n",
    "        trainingSetLoader = DataLoader(my_dataset,\n",
    "                shuffle=setShuffle(config),\n",
    "                sampler= upSamplingData(config),\n",
    "                batch_size=config[batchSize]) \n",
    "\n",
    "        tensor_x_test = torch.Tensor(X_test) \n",
    "        tensor_y_test = torch.Tensor(y_test)\n",
    "        my_dataset_test = TensorDataset(tensor_x_test,tensor_y_test) \n",
    "        testSetLoader = DataLoader(my_dataset_test,\n",
    "                shuffle=True,\n",
    "                batch_size=config[batchSize]) \n",
    "        return (trainingSetLoader, testSetLoader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, config[fc_size_1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[fc_size_1], config[fc_size_2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[fc_size_2], config[fc_size_3]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[fc_size_3], config[fc_size_4]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(config[fc_size_4], config[fc_size_5]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[fc_size_5], config[fc_size_6]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[fc_size_6], output_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training method.\n",
    "This method takes care of a single training pass. Another function call this one multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpoch(dataloader, model, criterion, optimizer, config):\n",
    "    # Metrics for training.\n",
    "    metricsResults = getMetricsDict(config)\n",
    "\n",
    "    # Enable the grad, for training.\n",
    "    with torch.set_grad_enabled(True):\n",
    "\n",
    "        # Indicate that the model is going to be trained.\n",
    "        model.train()\n",
    "\n",
    "        # Loader len, for metrics calculation.\n",
    "        loaderLen = len(dataloader)\n",
    "\n",
    "        # Iterate the batches for training.\n",
    "        for batch in dataloader:\n",
    "            # Train the model.\n",
    "            # Get the inputs and labels, and move them to the selected device.\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            # Zero the gradient parameters.\n",
    "            optimizer.zero_grad()\n",
    "            # Get the predictions.\n",
    "            outputs = model(inputs)\n",
    "            # Calculate the error.\n",
    "            labels_argmax = torch.empty(labels.shape[0], dtype=torch.long)\n",
    "            labels_argmax = torch.argmax(labels, dim=1)\n",
    "            loss = criterion(outputs, labels_argmax)\n",
    "            # Calculates the derivatives of the parameters that have a gradient.\n",
    "            loss.backward()\n",
    "            # Update the parameters based on the computer gradient.\n",
    "            optimizer.step()\n",
    "            # Metrics for the training set.\n",
    "            updateRunningMetrics(outputs, labels, loss, loaderLen, metricsResults, config)\n",
    "\n",
    "    return metricsResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation method.\n",
    "This method evaluates the model for a specified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, criterion, config):\n",
    "    # Metrics for testing.\n",
    "    metricsResults = getMetricsDict(config)\n",
    "\n",
    "    # Enable the grad, for training.\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        # Indicate that the model is going to be evaluated.\n",
    "        model.eval()\n",
    "\n",
    "        # Loader len, for metrics calculation.\n",
    "        loaderLen = len(dataloader)\n",
    "\n",
    "        # Iterate the batches for testing.\n",
    "        for batch in dataloader:\n",
    "            # Test the model.\n",
    "            # Get the inputs and labels, and move them to the selected device.\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            # Get the predictions.\n",
    "            outputs = model(inputs)\n",
    "            # Calculate the error.\n",
    "            labels_argmax = torch.empty(labels.shape[0], dtype=torch.long)\n",
    "            labels_argmax = torch.argmax(labels, dim=1)\n",
    "            loss = criterion(outputs, labels_argmax)\n",
    "            # Metrics for the testing set.\n",
    "            updateRunningMetrics(outputs, labels, loss, loaderLen, metricsResults, config)\n",
    "\n",
    "    return metricsResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainining and evaluate method.\n",
    "For the specific purpose of this project, in each epoch we evaluate metrics for each data set (training and testing) in each epoch, this method simplifies the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndEvaluate(trainloader, testloader, model, criterion, optimizer, config):\n",
    "    startTimeTotal = time.time()\n",
    "    print(\"Running epochs: \" + str(config[epochs]))\n",
    "    for epoch in range(1, config[epochs] + 1):\n",
    "        \n",
    "        # Train the model.\n",
    "        trainMetricsResults = trainEpoch(trainloader, model, criterion, optimizer, config)\n",
    "        processRunningMetrics(trainMetricsResults)\n",
    "\n",
    "        # Evaluate the model.\n",
    "        testMetricsResults = evaluate(testloader, model, criterion, config)\n",
    "        processRunningMetrics(testMetricsResults)\n",
    "\n",
    "        # Log on wandb\n",
    "        logMetricsWandb(trainMetricsResults, testMetricsResults, config)\n",
    "\n",
    "        # Print the results.\n",
    "        if epoch %20 == 0:\n",
    "            print('**', '[', 'Epoch ', epoch, ']', '*' * 48, sep='')\n",
    "            print('\\tTraining results:', end=' ')\n",
    "            printMetricsDict(trainMetricsResults, config)\n",
    "            print('\\t Testing results:', end=' ')\n",
    "            printMetricsDict(testMetricsResults, config)\n",
    "        \n",
    "    # Print time\n",
    "    print('Epochs terminados')\n",
    "    print(\"--- %s seconds ---\" % (time.time() - startTimeTotal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running\n",
      "WithoutUpsampling_32Batch\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2f9ymg3k) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11352<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>c:\\Users\\TEC\\Documents\\GitHub\\ML\\Proyectos\\Proyecto II\\wandb\\run-20210510_141817-2f9ymg3k\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>c:\\Users\\TEC\\Documents\\GitHub\\ML\\Proyectos\\Proyecto II\\wandb\\run-20210510_141817-2f9ymg3k\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>_runtime</td><td>331</td></tr><tr><td>_timestamp</td><td>1620678228</td></tr><tr><td>_step</td><td>59</td></tr><tr><td>Loss (training)</td><td>1.1495</td></tr><tr><td>Accuracy (training)</td><td>0.59219</td></tr><tr><td>COVID accuracy (training)</td><td>0.31499</td></tr><tr><td>Lung Opacity accuracy (training)</td><td>0.51457</td></tr><tr><td>Normal accuracy (training)</td><td>0.81521</td></tr><tr><td>Viral Pneumonia accuracy (training)</td><td>0.0</td></tr><tr><td>Loss (testing)</td><td>1.13992</td></tr><tr><td>Accuracy (testing)</td><td>0.60076</td></tr><tr><td>COVID accuracy (testing)</td><td>0.32325</td></tr><tr><td>Lung Opacity accuracy (testing)</td><td>0.56882</td></tr><tr><td>Normal accuracy (testing)</td><td>0.79432</td></tr><tr><td>Viral Pneumonia accuracy (testing)</td><td>0.0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>Loss (training)</td><td>█▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>Accuracy (training)</td><td>▁▁▁▁▁▁▁▁▃▄▅▆▆▇▇▇▇███</td></tr><tr><td>COVID accuracy (training)</td><td>▁▁▁▁▁▁▁▁▁▁▁▃▄▇▇█████</td></tr><tr><td>Lung Opacity accuracy (training)</td><td>▁▁▁▁▁▁▁▁▄▆▇██▇██████</td></tr><tr><td>Normal accuracy (training)</td><td>████████▅▃▂▁▁▂▁▁▁▁▁▁</td></tr><tr><td>Viral Pneumonia accuracy (training)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Loss (testing)</td><td>██▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>Accuracy (testing)</td><td>▁▁▁▁▁▁▁▂▄▅▅▆▇▇▇▇████</td></tr><tr><td>COVID accuracy (testing)</td><td>▁▁▁▁▁▁▁▁▁▁▁▃▆▆▇█▇▇██</td></tr><tr><td>Lung Opacity accuracy (testing)</td><td>▁▁▁▁▁▁▁▃▅▆█▇▇█▇██▇▇▇</td></tr><tr><td>Normal accuracy (testing)</td><td>███████▇▅▃▁▂▂▂▂▁▂▂▂▂</td></tr><tr><td>Viral Pneumonia accuracy (testing)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 41 media file(s), 28 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">WithoutUpsampling_32Batch1</strong>: <a href=\"https://wandb.ai/tecai/MLP/runs/2f9ymg3k\" target=\"_blank\">https://wandb.ai/tecai/MLP/runs/2f9ymg3k</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:2f9ymg3k). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">WithoutUpsampling_32Batch2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/tecai/MLP\" target=\"_blank\">https://wandb.ai/tecai/MLP</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/tecai/MLP/runs/2sqra8vi\" target=\"_blank\">https://wandb.ai/tecai/MLP/runs/2sqra8vi</a><br/>\n",
       "                Run data is saved locally in <code>c:\\Users\\TEC\\Documents\\GitHub\\ML\\Proyectos\\Proyecto II\\wandb\\run-20210510_142641-2sqra8vi</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epochs: 21\n",
      "**[Epoch 20]************************************************\n",
      "\tTraining results: "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "printMetricsDict() missing 1 required positional argument: 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-243d81e737d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m#Train and evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mtrainAndEvaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunConfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;31m#Finish WandB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-7ddbce9c9aa8>\u001b[0m in \u001b[0;36mtrainAndEvaluate\u001b[1;34m(trainloader, testloader, model, criterion, optimizer, config)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'**'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'['\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Epoch '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m']'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m48\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\tTraining results:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mprintMetricsDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainMetricsResults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t Testing results:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mprintMetricsDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestMetricsResults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: printMetricsDict() missing 1 required positional argument: 'config'"
     ]
    }
   ],
   "source": [
    "#Create the model, optimizer and criterion\n",
    "run_ID += 1\n",
    "for runConfig in configs:\n",
    "    print(\"Running\")\n",
    "    print(runConfig[name])\n",
    "    trainloader, testloader = createDataLoaders(runConfig)\n",
    "    \n",
    "    model = MLP(runConfig[inputSize], runConfig[outputSize], runConfig)\n",
    "    optimizer = optim.Adam(model.parameters(),lr=runConfig[learningRate])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    #Init WandB\n",
    "    nameRun = runConfig[name] + str(run_ID)\n",
    "    run = wandb.init(project='MLP', entity='tecai', config=runConfig,\n",
    "                            name=nameRun)\n",
    "    wandb.watch(model)\n",
    "    #Train and evaluate the model\n",
    "    trainAndEvaluate(trainloader, testloader, model, criterion, optimizer, runConfig)\n",
    "    #Finish WandB\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "3a31fab793c547cb35fe033c429e016f6dd189ef070db975037acd85972e51d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
