{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd009cf3d5909f3828bf53c46564821f879a9c8405e40e27fb09b54d745264e4a6d",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General import of torch.\n",
    "import torch\n",
    "# Import for graph blocks of torch.\n",
    "import torch.nn as nn\n",
    "# Import the models library, to get the model to be used.\n",
    "import torchvision.models as models\n",
    "# Import optim library, to get the optimizer to be used.\n",
    "import torch.optim as optim\n",
    "# Import torchvision, to manage the input data.\n",
    "import torchvision\n",
    "# To apply transformations to the data (when loaded).\n",
    "import torchvision.transforms as transforms\n",
    "# Create a Dataset.\n",
    "from torch.utils.data import Dataset\n",
    "# To calculate softmax.\n",
    "from torch.nn import Softmax\n",
    "softmax = Softmax(dim=1)\n",
    "\n",
    "# Metrics.\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# General imports.\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import wandb\n",
    "import random\n",
    "import natsort\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from sklearn import manifold\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "source": [
    "### Enviroment configuration."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input size of the model.\n",
    "inputSize = 'inputSize'\n",
    "# Output size of the model.\n",
    "outputSize = 'outputSize'\n",
    "# Batch size.\n",
    "batchSize = 'batchSize'\n",
    "# Epoch init.\n",
    "epochsInit = 'epochInit'\n",
    "# Epochs amount.\n",
    "epochs = 'epochs'\n",
    "# Class names.\n",
    "classes = 'classes'\n",
    "# Class IDs.\n",
    "classesID = 'classesIDs'\n",
    "# Number of classes to classify.\n",
    "classesLen = 'classesLen'\n",
    "# Group type, for wandb.\n",
    "groupType = 'groupType'\n",
    "\n",
    "config = {\n",
    "    inputSize    : 224,\n",
    "    outputSize   : 39,\n",
    "    batchSize    : 32,\n",
    "    epochsInit   : 1,\n",
    "    epochs       : 50,\n",
    "    classes : [\n",
    "        'Apple - Apple scab',\n",
    "        'Apple - Black rot',\n",
    "        'Apple - Cedar apple rust',\n",
    "        'Apple - Healthy',\n",
    "        'Background without leaves',\n",
    "        'Blueberry - Healthy',\n",
    "        'Cherry - Healthy',\n",
    "        'Cherry - Powdery mildew',\n",
    "        'Corn - Cercospora',\n",
    "        'Corn - Common rust',\n",
    "        'Corn - Healthy',\n",
    "        'Corn - Northern Leaf Blight',\n",
    "        'Grape - Black rot',\n",
    "        'Grape - Esca',\n",
    "        'Grape - Healthy',\n",
    "        'Grape - Leaf blight',\n",
    "        'Orange - Haunglongbing',\n",
    "        'Peach - Bacterial spot',\n",
    "        'Peach - Healthy',\n",
    "        'Pepper bell - Bacterial spot',\n",
    "        'Pepper bell - healthy',\n",
    "        'Potato - Early blight',\n",
    "        'Potato - Healthy',\n",
    "        'Potato - Late blight',\n",
    "        'Raspberry - healthy',\n",
    "        'Soybean - Healthy',\n",
    "        'Squash - Powdery mildew',\n",
    "        'Strawberry - Healthy',\n",
    "        'Strawberry - Leaf scorch',\n",
    "        'Tomato - Bacterial spot',\n",
    "        'Tomato - Early blight',\n",
    "        'Tomato - Healthy',\n",
    "        'Tomato - Late blight',\n",
    "        'Tomato - Leaf Mold',\n",
    "        'Tomato - Septoria leaf spot',\n",
    "        'Tomato - Spider mites',\n",
    "        'Tomato - Target Spot',\n",
    "        'Tomato - Mosaic virus',\n",
    "        'Tomato - Yellow Leaf Curl Virus'\n",
    "    ],\n",
    "    classesID : [i + 1 for i in range(39)],\n",
    "    classesLen : 39\n",
    "}\n",
    "\n",
    "# Device to be used, prefer cuda, if available.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About metrics.\n",
    "# Metric dictionary keys.\n",
    "# For preprocessing.\n",
    "_loss        = 'Loss'\n",
    "# For torch save\n",
    "_model           = 'Model State Dic'\n",
    "_optimizer       = 'Optimizer State Dic'\n",
    "_criterion       = 'Criterion'\n",
    "_epoch           = 'Epoch'\n",
    "\n",
    "# Get a clean dictionary for the metrics.\n",
    "def getMetricsDict():\n",
    "    return {\n",
    "        _loss : torch.tensor(0.)\n",
    "    }\n",
    "\n",
    "# Function used to update the dictionary of resulting metrics.\n",
    "def updateRunningMetrics(logits, groundtruth, loss, batchAmount, metricsResults):\n",
    "    # Accumulate the loss.\n",
    "    metricsResults[_loss] += loss / batchAmount\n",
    "\n",
    "# Function used to process the dictionary of resulting metrics (make final calculations).\n",
    "def processRunningMetrics(metricsResults):\n",
    "    # Detach the other values in the dictionary.\n",
    "    metricsResults[_loss] = metricsResults[_loss].detach()\n",
    "\n",
    "# Pretty print the metrics dictionaries.\n",
    "def printMetricsDict(metricsResults):\n",
    "    # All metrics to print\n",
    "    metricPrints = []\n",
    "\n",
    "    # Format the loss.\n",
    "    lossPrint = 'Loss: {:.5f}'.format(metricsResults[_loss])\n",
    "    metricPrints.append(lossPrint)\n",
    "\n",
    "    print(', '.join(metricPrints), end='')\n",
    "\n",
    "# This functions process an metrics result dictionary for wandb. Is necessary to indicte\n",
    "#   the metrics origin, training or testing.\n",
    "def processMetricsWandb(metricsResults, training=False):\n",
    "    # Get the prefix to log on wandb, the keys must be different.\n",
    "    resultsType = 'training' if training else 'testing'\n",
    "\n",
    "    # All the wandb keys are based in the original metrics results keys.\n",
    "    lossKey = '{} ({})'.format(_loss, resultsType)\n",
    "\n",
    "    # Make the dictionary for wandb and store the values.\n",
    "    wandbDict = {\n",
    "        lossKey : metricsResults[_loss].item()\n",
    "    }\n",
    "\n",
    "    # Return, to log later.\n",
    "    return wandbDict\n",
    "\n",
    "# Get the metrics dictionaries for wandb and log them.\n",
    "def logMetricsWandb(trainMetricsResults, testMetricsResults):\n",
    "    # Get both dictionaries for wandb.\n",
    "    wandbTrainDict = processMetricsWandb(trainMetricsResults, training=True)\n",
    "    wandbTestDict  = processMetricsWandb(testMetricsResults, training=False)\n",
    "\n",
    "    # Merge the dictionaries.\n",
    "    wandbDict = {**wandbTrainDict, **wandbTestDict}\n",
    "\n",
    "    # Log on wandb\n",
    "    wandb.log(wandbDict)\n",
    "\n",
    "# Function used to save the model and the metrics.\n",
    "def saveEpochData(model, optimizer, criterion, epoch, rootPath):\n",
    "    # Create a dir for the current epoch.\n",
    "    Path(rootPath).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Path\n",
    "    savePath = os.path.join(rootPath, 'model.pth')\n",
    "\n",
    "    # Make dict for torch.save\n",
    "    saveDict = {\n",
    "        _model     : model.state_dict(),\n",
    "        _optimizer : optimizer.state_dict(),\n",
    "        _criterion : criterion,\n",
    "        _epoch     : epoch\n",
    "    }\n",
    "    \n",
    "    # Save\n",
    "    torch.save(saveDict, savePath)"
   ]
  },
  {
   "source": [
    "### Loader function.\n",
    "Should return the training loader and test loader, a iterable object by batches."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, main_dir, transform):\n",
    "        self.main_dir = main_dir\n",
    "        self.transform = transform\n",
    "        self.total_imgs = [f for f in glob.glob(os.path.join(main_dir, \"**\", \"*.jpeg\"), recursive=True)]\n",
    "        #all_imgs = os.listdir(main_dir)\n",
    "        #self.total_imgs = natsort.natsorted(all_imgs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n",
    "        image = Image.open(img_loc).convert(\"RGB\")\n",
    "        tensor_image = self.transform(image)\n",
    "        return (tensor_image, tensor_image)\n",
    "\n",
    "# Transformation definitions.\n",
    "transformTrain = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(config[inputSize]),  # This one does a resize (it cuts randomly, it doesn't keep the whole image).\n",
    "        transforms.RandomHorizontalFlip(),                # Flip the image horizontally randomly.\n",
    "        transforms.ToTensor()                             # Make the image a tensor.\n",
    "    ])\n",
    "transformTest = transforms.Compose([\n",
    "        transforms.Resize(config[inputSize]),             # Resize the image, keeping all pixels.\n",
    "        transforms.CenterCrop(config[inputSize]),         # Cut the image in the center.\n",
    "        transforms.ToTensor()                             # Make the image a tensor.\n",
    "    ])\n",
    "\n",
    "# Function used to get the data loaders.\n",
    "# A folder with two folders inside called train and test is expected as a rootPath.\n",
    "def getLoaders(trainPath, testPath):\n",
    "\n",
    "    # Get the training and test data, apply the transformations.\n",
    "    trainset = CustomDataSet(trainPath, transformTrain)\n",
    "    testset  = CustomDataSet(testPath,  transformTest)\n",
    "\n",
    "    # Get the loaders, to iterate the data through batches.\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=config[batchSize], shuffle=True, num_workers=2)\n",
    "    testloader  = torch.utils.data.DataLoader(testset,  batch_size=config[batchSize], shuffle=False, num_workers=2)\n",
    "    \n",
    "    return trainloader, testloader"
   ]
  },
  {
   "source": [
    "# Network definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Function used to create a coder."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_coder(channels, kernel_sizes, strides, conv_types, activation_types, paddings=(0,0), batch_norms=False):\n",
    "    '''\n",
    "    Function that creates en- or decoders based on parameters\n",
    "    Args:\n",
    "        channels ([int]): Channel sizes per layer. 1 more than layers\n",
    "        kernel_sizes ([int]): Kernel sizes per layer\n",
    "        strides ([int]): Strides per layer\n",
    "        conv_types ([f()->type]): Type of the convoultion module per layer\n",
    "        activation_types ([f()->type]): Type of activation function per layer\n",
    "        paddings ([(int, int)]): The padding per layer\n",
    "        batch_norms ([bool]): Whether to use batchnorm on each layer\n",
    "    Returns (nn.Sequential): The created coder\n",
    "    '''\n",
    "    if not isinstance(conv_types, list):\n",
    "        conv_types = [conv_types for _ in range(len(kernel_sizes))]\n",
    "\n",
    "    if not isinstance(activation_types, list):\n",
    "        activation_types = [activation_types for _ in range(len(kernel_sizes))]\n",
    "\n",
    "    if not isinstance(paddings, list):\n",
    "        paddings = [paddings for _ in range(len(kernel_sizes))]\n",
    "        \n",
    "    if not isinstance(batch_norms, list):\n",
    "        batch_norms = [batch_norms for _ in range(len(kernel_sizes))]\n",
    "\n",
    "    coder = nn.Sequential()\n",
    "    for layer in range(len(channels)-1):\n",
    "        coder.add_module(\n",
    "            'conv'+ str(layer), \n",
    "            conv_types[layer](\n",
    "                in_channels=channels[layer], \n",
    "                out_channels=channels[layer+1],\n",
    "                kernel_size=kernel_sizes[layer],\n",
    "                stride=strides[layer]\n",
    "            )\n",
    "        )\n",
    "        if batch_norms[layer]:\n",
    "            coder.add_module(\n",
    "                'norm'+str(layer),\n",
    "                nn.BatchNorm2d(channels[layer+1])\n",
    "            )\n",
    "        if not activation_types[layer] is None:\n",
    "            coder.add_module('acti'+str(layer),activation_types[layer]())\n",
    "\n",
    "    return coder\n"
   ]
  },
  {
   "source": [
    "### VAE definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemplateVAE(nn.Module):\n",
    "    '''\n",
    "    A template class for Variational Autoencoders to minimize code duplication\n",
    "    Args:\n",
    "        input_size (int,int): The height and width of the input image\n",
    "        z_dimensions (int): The number of latent dimensions in the encoding\n",
    "        variational (bool): Whether the model is variational or not\n",
    "        gamma (float): The weight of the KLD loss\n",
    "        perceptual_net: Which perceptual network to use (None for pixel-wise)\n",
    "    '''\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        mu = self.mu(x)\n",
    "        logvar = self.logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def sample(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.autograd.Variable(std.data.new(std.size()).normal_())\n",
    "        out = eps.mul(std).add_(mu)\n",
    "        return out\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        if self.variational:\n",
    "            z = self.sample(mu, logvar)\n",
    "        else:\n",
    "            z = mu\n",
    "        rec_x = self.decode(z)\n",
    "        return rec_x, z, mu, logvar\n",
    "\n",
    "    def loss(self, output, x):\n",
    "        rec_x, z, mu, logvar = output\n",
    "        if self.perceptual_loss:\n",
    "            x = self.perceptual_net(x)\n",
    "            rec_x = self.perceptual_net(rec_x)\n",
    "        else:\n",
    "            x = x.reshape(x.size(0), -1)\n",
    "            rec_x = rec_x.view(x.size(0), -1)\n",
    "            \n",
    "        REC = nn.functional.mse_loss(rec_x, x, reduction='mean')\n",
    "\n",
    "        if self.variational:\n",
    "            KLD = -1 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            return REC + self.gamma*KLD, REC, KLD\n",
    "        else:\n",
    "            return [REC]\n",
    "\n",
    "class FourLayerCVAE(TemplateVAE):\n",
    "    '''\n",
    "    A Convolutional Variational Autoencoder for images\n",
    "    Args:\n",
    "        input_size (int,int): The height and width of the input image\n",
    "            acceptable sizes are 64+16*n\n",
    "        z_dimensions (int): The number of latent dimensions in the encoding\n",
    "        variational (bool): Whether the model is variational or not\n",
    "        gamma (float): The weight of the KLD loss\n",
    "        perceptual_net: Which perceptual network to use (None for pixel-wise)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_size=(64,64), z_dimensions=32,\n",
    "        variational=True, gamma=20.0, perceptual_net=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        #Parameter check\n",
    "        if (input_size[0] - 64) % 16 != 0 or (input_size[1] - 64) % 16 != 0:\n",
    "            raise ValueError(\n",
    "                f'Input_size is {input_size}, but must be 64+16*N'\n",
    "            )\n",
    "\n",
    "        #Attributes\n",
    "        self.input_size = input_size\n",
    "        self.z_dimensions = z_dimensions\n",
    "        self.variational = variational\n",
    "        self.gamma = gamma\n",
    "        self.perceptual_net = perceptual_net\n",
    "\n",
    "        self.perceptual_loss = not perceptual_net is None\n",
    "            \n",
    "        encoder_channels = [3,32,64,128,256]\n",
    "        self.encoder = _create_coder(\n",
    "            encoder_channels, [4,4,4,4], [2,2,2,2],\n",
    "            nn.Conv2d, nn.ReLU,\n",
    "            batch_norms=[True,True,True,True]\n",
    "        )\n",
    "        \n",
    "        f = lambda x: np.floor((x - (2,2))/2)\n",
    "        conv_sizes = f(f(f(f(np.array(input_size)))))\n",
    "        conv_flat_size = int(encoder_channels[-1]*conv_sizes[0]*conv_sizes[1])\n",
    "        self.mu = nn.Linear(conv_flat_size, self.z_dimensions)\n",
    "        self.logvar = nn.Linear(conv_flat_size, self.z_dimensions)\n",
    "\n",
    "        g = lambda x: int((x-64)/16)+1\n",
    "        deconv_flat_size = g(input_size[0]) * g(input_size[1]) * 1024\n",
    "        self.dense = nn.Linear(self.z_dimensions, deconv_flat_size)\n",
    "\n",
    "        self.decoder = _create_coder(\n",
    "            [1024,128,64,32,3], [5,5,6,6], [2,2,2,2],\n",
    "            nn.ConvTranspose2d,\n",
    "            [nn.ReLU,nn.ReLU,nn.ReLU,nn.Sigmoid],\n",
    "            batch_norms=[True,True,True,False]\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def decode(self, z):\n",
    "        y = self.dense(z)\n",
    "        y = self.relu(y)\n",
    "        y = y.view(\n",
    "            y.size(0), 1024,\n",
    "            int((self.input_size[0]-64)/16)+1,\n",
    "            int((self.input_size[1]-64)/16)+1\n",
    "        )\n",
    "        y = self.decoder(y)\n",
    "        return y"
   ]
  },
  {
   "source": [
    "### Perceptual net creation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_features = {\n",
    "    'alexnet' : ['features'],\n",
    "    'vgg16' : ['features'],\n",
    "}\n",
    "\n",
    "class SimpleExtractor(nn.Module):\n",
    "    '''\n",
    "    A simple feature extractor for torchvision models\n",
    "    Args:\n",
    "        architecture (str): The architecture to extract from\n",
    "        layer (int): The sub-module in 'features' to extract at\n",
    "        frozen (bool): Whether the network can be trained\n",
    "        sigmoid_out (bool): Whether to normalize the output with a sigmoid\n",
    "    '''\n",
    "    def __init__(self, architecture, layer, frozen=True, sigmoid_out=True):\n",
    "        super(SimpleExtractor, self).__init__()\n",
    "        self.architecture = architecture\n",
    "        self.layer = layer\n",
    "        self.frozen = frozen\n",
    "        self.sigmoid_out = sigmoid_out\n",
    "    \n",
    "        os.environ['TORCH_HOME'] = './'\n",
    "        original_model = models.__dict__[architecture](pretrained=True)\n",
    "        original_features = original_model\n",
    "        for attribute in architecture_features[architecture]:\n",
    "            original_features = getattr(original_features, attribute)\n",
    "        self.features = nn.Sequential(\n",
    "            *list(original_features.children())[:layer]\n",
    "        )\n",
    "        if sigmoid_out:\n",
    "            self.features.add_module('sigmoid',nn.Sigmoid())\n",
    "        if frozen:\n",
    "            self.eval()\n",
    "            for param in self.features.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "def AlexNet(layer=5, pretrained=True, frozen=True, sigmoid_out=True):\n",
    "    return SimpleExtractor('alexnet', layer, frozen, sigmoid_out)\n",
    "\n",
    "def VGG16(layer=5, pretrained=True, frozen=True, sigmoid_out=True):\n",
    "    return SimpleExtractor('vgg16', layer, frozen, sigmoid_out)"
   ]
  },
  {
   "source": [
    "### Epoch progress"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epochProgress(currentBatch, totalBatch, loss, time, type):\n",
    "    # Base string.\n",
    "    line = '\\r{} - [{}/{}] - Losses: {}, Time elapsed: {}s'\n",
    "    # Time string.\n",
    "    elapsedTime = '{0:.1f}'.format(time)\n",
    "    # Loss.\n",
    "    actualLoss = '{0:.5f}'.format(loss)\n",
    "    print(line.format(type, currentBatch, totalBatch, actualLoss, elapsedTime), end='')"
   ]
  },
  {
   "source": [
    "### Training method.\n",
    "This method takes care of a single training pass. Another function call this one multiple times."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpoch(dataloader, model, criterion, optimizer):\n",
    "    # Get init time.\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Metrics for training.\n",
    "    metricsResults = getMetricsDict()\n",
    "\n",
    "    # Enable the grad, for training.\n",
    "    with torch.set_grad_enabled(True):\n",
    "\n",
    "        # Indicate that the model is going to be trained.\n",
    "        model.train()\n",
    "\n",
    "        # Loader len, for metrics calculation.\n",
    "        loaderLen = len(dataloader)\n",
    "\n",
    "        # Iterate the batches for training.\n",
    "        for i, batch in enumerate(dataloader):\n",
    "\n",
    "            # Train the model.\n",
    "            # Get the inputs and labels, and move them to the selected device.\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "            # Zero the gradient parameters.\n",
    "            optimizer.zero_grad()\n",
    "            # Get the predictions.\n",
    "            outputs = model(inputs)\n",
    "            # Calculate the error.\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Calculates the derivatives of the parameters that have a gradient.\n",
    "            loss[0].backward()\n",
    "            # Update the parameters based on the computer gradient.\n",
    "            optimizer.step()\n",
    "            # Metrics for the training set.\n",
    "            updateRunningMetrics(outputs, labels, loss[0].item(), loaderLen, metricsResults)\n",
    "\n",
    "            epochProgress(i + 1, loaderLen, loss[0].item(), time.time() - start_time, 'Training')\n",
    "\n",
    "    return metricsResults"
   ]
  },
  {
   "source": [
    "### Evaluation method.\n",
    "This method evaluates the model for a specified dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, criterion):\n",
    "    # Get init time.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Metrics for testing.\n",
    "    metricsResults = getMetricsDict()\n",
    "\n",
    "    # Enable the grad, for training.\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        # Indicate that the model is going to be evaluated.\n",
    "        model.eval()\n",
    "\n",
    "        # Loader len, for metrics calculation.\n",
    "        loaderLen = len(dataloader)\n",
    "\n",
    "        # Iterate the batches for testing.\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # Test the model.\n",
    "            # Get the inputs and labels, and move them to the selected device.\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            # Get the predictions.\n",
    "            outputs = model(inputs)\n",
    "            # Calculate the error.\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Metrics for the testing set.\n",
    "            updateRunningMetrics(outputs, labels, loss[0].item(), loaderLen, metricsResults)\n",
    "\n",
    "            epochProgress(i, loaderLen, loss[0].item(), time.time() - start_time, 'Evaluating')\n",
    "\n",
    "    return metricsResults"
   ]
  },
  {
   "source": [
    "### Display method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPredictions(loader, model, folder, epoch, saveOriginal):\n",
    "    # Enable the grad, for training.\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        # Indicate that the model is going to be evaluated.\n",
    "        model.eval()\n",
    "        \n",
    "        # Get a batch from the loader.\n",
    "        firstBatch = next(iter(loader))\n",
    "\n",
    "        # Get the inputs and labels, and move them to the selected device.\n",
    "        inputs, labels = firstBatch[0].to(device), firstBatch[1].to(device)\n",
    "        # Get the predictions.\n",
    "        rec_x, z, mu, logvar = model(inputs)\n",
    "\n",
    "        torchvision.utils.save_image(rec_x, f\"{folder}/{epoch}_pred.png\")\n",
    "        if saveOriginal:\n",
    "            torchvision.utils.save_image(labels, f\"{folder}/{epoch}_orig.png\")\n",
    "            "
   ]
  },
  {
   "source": [
    "### Trainining and evaluate method.\n",
    "For the specific purpose of this project, in each epoch we evaluate metrics for each data set (training and testing) in each epoch, this method simplifies the process. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndEvaluate(trainloader, testloader, model, criterion, optimizer, display, savePath):\n",
    "\n",
    "    originalDisplayed = False\n",
    "\n",
    "    for epoch in range(config[epochsInit], config[epochs] + 1):\n",
    "        \n",
    "        # Train the model.\n",
    "        trainMetricsResults = trainEpoch(trainloader, model, criterion, optimizer)\n",
    "        processRunningMetrics(trainMetricsResults)\n",
    "\n",
    "        # Evaluate the model.\n",
    "        testMetricsResults = evaluate(testloader, model, criterion)\n",
    "        processRunningMetrics(testMetricsResults)\n",
    "\n",
    "        # Display some random images.\n",
    "        if display is not None:\n",
    "            showPredictions(trainloader, model, display, epoch, not originalDisplayed)\n",
    "            #originalDisplayed = True\n",
    "\n",
    "        # Log on wandb\n",
    "        logMetricsWandb(trainMetricsResults, testMetricsResults)\n",
    "\n",
    "        # Display latent vectors in images.\n",
    "        if epoch % 5 == 0:\n",
    "            runTSNE(model, epoch, folder=\"tsneImages/\")\n",
    "\n",
    "        # Save model and metrics for the epochs.\n",
    "        saveEpochData(model, optimizer, criterion, epoch, savePath)\n",
    "\n",
    "        # Print the results.\n",
    "        if epoch % 1 == 0:\n",
    "            print('\\r**', '[', 'Epoch ', epoch, ']', '*' * 48, sep='')\n",
    "            print('\\tTraining', end=' ')\n",
    "            printMetricsDict(trainMetricsResults)\n",
    "            print(end=' | ')\n",
    "            print('Evaluation', end=' ')\n",
    "            printMetricsDict(testMetricsResults)\n",
    "            print()"
   ]
  },
  {
   "source": [
    "### Function used to visualize the latent vector with TSNE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveTSNE(loader, model, epoch, folder):\n",
    "    # Get init time.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Enable the grad, for training.\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        # Indicate that the model is going to be evaluated.\n",
    "        model.eval()\n",
    "\n",
    "        pred_acc = torch.tensor(()).to(device)\n",
    "        labels_acc = torch.tensor(()).to(device)\n",
    "\n",
    "        top = len(loader)\n",
    "\n",
    "        for i, (images, labels) in enumerate(loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            rec_x, z, mu, logvar = model(images)\n",
    "\n",
    "            #print(z.size())\n",
    "            #input()\n",
    "\n",
    "            pred_acc = torch.cat((pred_acc, z), 0)\n",
    "            labels_acc = torch.cat((labels_acc, labels), 0)\n",
    "\n",
    "            if i == top:\n",
    "                break\n",
    "\n",
    "            epochProgress(i + 1, top, 0, time.time() - start_time, 'TSNE')\n",
    "\n",
    "        print(end=' fitting TSNE,')\n",
    "        tsne = TSNE(n_components=3, verbose=0, perplexity=40, n_iter=1500)\n",
    "\n",
    "        pred_acc = pred_acc.to(\"cpu\").detach().numpy()\n",
    "        Y = tsne.fit_transform(pred_acc)\n",
    "        labels_acc = labels_acc.to(\"cpu\")\n",
    "        print(end=' done,')\n",
    "\n",
    "        fig = plt.figure(figsize=(12.8*1.5, 9.6*1.5))\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "        print(end=' plotting image.')\n",
    "        ax.scatter(Y[:, 0], Y[:, 1], Y[:, 2], c=labels_acc)\n",
    "\n",
    "        plt.savefig(f\"{folder}/epoch_TSNE_epoch_{epoch}.png\")\n",
    "\n",
    "def runTSNE(model, epoch, folder=\"tsneImages/\"):\n",
    "    dataPathTest = '/home/pablo/Desktop/ML/Proyectos/Proyecto III/data/corrida1/labelTest'\n",
    "\n",
    "    transformTest = transforms.Compose([\n",
    "            transforms.Resize(config[inputSize]),             # Resize the image, keeping all pixels.\n",
    "            transforms.CenterCrop(config[inputSize]),         # Cut the image in the center.\n",
    "            transforms.ToTensor()                             # Make the image a tensor.\n",
    "        ])\n",
    "\n",
    "    testset  = torchvision.datasets.ImageFolder(dataPathTest, transformTest)\n",
    "\n",
    "    # Get the loaders, to iterate the data through batches.\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=config[batchSize], shuffle=True, num_workers=2)\n",
    "\n",
    "    saveTSNE(testloader, model, epoch, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCheckpoint(model, optimizer, criterion, loadPath):\n",
    "    # Load the checkpoint.\n",
    "    checkpoint = torch.load(loadPath)\n",
    "    # Load model.\n",
    "    model.load_state_dict(checkpoint[_model])\n",
    "    # Load optimizer.\n",
    "    optimizer.load_state_dict(checkpoint[_optimizer])\n",
    "    # Load criterion.\n",
    "    criterion = checkpoint[_criterion]\n",
    "    # Set epoch.\n",
    "    config[epochsInit] = checkpoint[_epoch] + 1\n",
    "\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "def executeTest(network, dataPath, runName, savePath, display=None, loadPath=None):\n",
    "    # Get the model.\n",
    "    # Get a predefined model from pytorch, without the pretrained parameters.\n",
    "    net = network\n",
    "\n",
    "    # Load the model to the selected device.\n",
    "    net.to(device)\n",
    "    \n",
    "    # Get criterion and optimizer.\n",
    "    # Optimizer and the loss funtion used to train the model.\n",
    "    criterion = network.loss\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "    # Check if required to load a checkpoint.\n",
    "    if loadPath != None:\n",
    "        net, optimizer, criterion = loadCheckpoint(net, optimizer, criterion, loadPath)\n",
    "        net.to(device)\n",
    "\n",
    "    # Get the loaders.\n",
    "    trainloader, testloader = getLoaders(dataPath[0], dataPath[1])\n",
    "\n",
    "    # Init wandb\n",
    "    run = wandb.init(project='VAE', entity='tecai', config=config, name=runName)\n",
    "\n",
    "    # Train and evaluate\n",
    "    trainAndEvaluate(trainloader, testloader, net, criterion, optimizer, display, savePath)\n",
    "\n",
    "    # Finish wandb\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpablobrenes\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.31<br/>\n                Syncing run <strong style=\"color:#cdcd00\">dainty-snowflake-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/tecai/VAE\" target=\"_blank\">https://wandb.ai/tecai/VAE</a><br/>\n                Run page: <a href=\"https://wandb.ai/tecai/VAE/runs/m8iu1drm\" target=\"_blank\">https://wandb.ai/tecai/VAE/runs/m8iu1drm</a><br/>\n                Run data is saved locally in <code>/home/pablo/Desktop/ML/Pruebas/VAEPerceptualPlants/wandb/run-20210620_132539-m8iu1drm</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "**[Epoch 1]************************************************\n",
      "\tTraining Loss: 0.00543 | Evaluation Loss: 0.00520\n",
      "**[Epoch 2]************************************************\n",
      "\tTraining Loss: 0.00485 | Evaluation Loss: 0.00495\n",
      "**[Epoch 3]************************************************\n",
      "\tTraining Loss: 0.00452 | Evaluation Loss: 0.00448\n",
      "**[Epoch 4]************************************************\n",
      "\tTraining Loss: 0.00421 | Evaluation Loss: 0.00548\n",
      "**[Epoch 5]************************************************\n",
      "\tTraining Loss: 0.00410 | Evaluation Loss: 0.00432\n",
      "**[Epoch 6]************************************************\n",
      "\tTraining Loss: 0.00401 | Evaluation Loss: 0.00410\n",
      "**[Epoch 7]************************************************\n",
      "\tTraining Loss: 0.00398 | Evaluation Loss: 0.00397\n",
      "**[Epoch 8]************************************************\n",
      "\tTraining Loss: 0.00396 | Evaluation Loss: 0.00391\n",
      "**[Epoch 9]************************************************\n",
      "\tTraining Loss: 0.00392 | Evaluation Loss: 0.00398\n",
      "**[Epoch 10]************************************************\n",
      "\tTraining Loss: 0.00392 | Evaluation Loss: 0.00389\n",
      "**[Epoch 11]************************************************\n",
      "\tTraining Loss: 0.00388 | Evaluation Loss: 0.00417\n",
      "**[Epoch 12]************************************************\n",
      "\tTraining Loss: 0.00388 | Evaluation Loss: 0.00398\n",
      "**[Epoch 13]************************************************\n",
      "\tTraining Loss: 0.00389 | Evaluation Loss: 0.00383\n",
      "**[Epoch 14]************************************************\n",
      "\tTraining Loss: 0.00386 | Evaluation Loss: 0.00467\n",
      "**[Epoch 15]************************************************\n",
      "\tTraining Loss: 0.00386 | Evaluation Loss: 0.00442\n",
      "**[Epoch 16]************************************************\n",
      "\tTraining Loss: 0.00386 | Evaluation Loss: 0.00390\n",
      "**[Epoch 17]************************************************\n",
      "\tTraining Loss: 0.00383 | Evaluation Loss: 0.00389\n",
      "**[Epoch 18]************************************************\n",
      "\tTraining Loss: 0.00383 | Evaluation Loss: 0.00429\n",
      "**[Epoch 19]************************************************\n",
      "\tTraining Loss: 0.00383 | Evaluation Loss: 0.00382\n",
      "**[Epoch 20]************************************************\n",
      "\tTraining Loss: 0.00383 | Evaluation Loss: 0.00382\n",
      "**[Epoch 21]************************************************\n",
      "\tTraining Loss: 0.00382 | Evaluation Loss: 0.00381\n",
      "**[Epoch 22]************************************************\n",
      "\tTraining Loss: 0.00379 | Evaluation Loss: 0.00378\n",
      "**[Epoch 23]************************************************\n",
      "\tTraining Loss: 0.00377 | Evaluation Loss: 0.00382\n",
      "**[Epoch 24]************************************************\n",
      "\tTraining Loss: 0.00378 | Evaluation Loss: 0.00396\n",
      "**[Epoch 25]************************************************\n",
      "\tTraining Loss: 0.00376 | Evaluation Loss: 0.00374\n",
      "**[Epoch 26]************************************************\n",
      "\tTraining Loss: 0.00377 | Evaluation Loss: 0.00389\n",
      "**[Epoch 27]************************************************\n",
      "\tTraining Loss: 0.00377 | Evaluation Loss: 0.00379\n",
      "**[Epoch 28]************************************************\n",
      "\tTraining Loss: 0.00372 | Evaluation Loss: 0.00372\n",
      "**[Epoch 29]************************************************\n",
      "\tTraining Loss: 0.00374 | Evaluation Loss: 0.00371\n",
      "**[Epoch 30]************************************************\n",
      "\tTraining Loss: 0.00375 | Evaluation Loss: 0.00374\n",
      "**[Epoch 31]************************************************\n",
      "\tTraining Loss: 0.00371 | Evaluation Loss: 0.00367\n",
      "**[Epoch 32]************************************************\n",
      "\tTraining Loss: 0.00371 | Evaluation Loss: 0.00365\n",
      "**[Epoch 33]************************************************\n",
      "\tTraining Loss: 0.00366 | Evaluation Loss: 0.00366\n",
      "**[Epoch 34]************************************************\n",
      "\tTraining Loss: 0.00368 | Evaluation Loss: 0.00363\n",
      "**[Epoch 35]************************************************\n",
      "\tTraining Loss: 0.00368 | Evaluation Loss: 0.00361\n",
      "**[Epoch 36]************************************************\n",
      "\tTraining Loss: 0.00367 | Evaluation Loss: 0.00368\n",
      "**[Epoch 37]************************************************\n",
      "\tTraining Loss: 0.00365 | Evaluation Loss: 0.00359\n",
      "**[Epoch 38]************************************************\n",
      "\tTraining Loss: 0.00363 | Evaluation Loss: 0.00358\n",
      "**[Epoch 39]************************************************\n",
      "\tTraining Loss: 0.00364 | Evaluation Loss: 0.00363\n",
      "**[Epoch 40]************************************************\n",
      "\tTraining Loss: 0.00365 | Evaluation Loss: 0.00452\n",
      "**[Epoch 41]************************************************\n",
      "\tTraining Loss: 0.00393 | Evaluation Loss: 0.00369\n",
      "**[Epoch 42]************************************************\n",
      "\tTraining Loss: 0.00370 | Evaluation Loss: 0.00359\n",
      "**[Epoch 43]************************************************\n",
      "\tTraining Loss: 0.00366 | Evaluation Loss: 0.00358\n",
      "**[Epoch 44]************************************************\n",
      "\tTraining Loss: 0.00363 | Evaluation Loss: 0.00358\n",
      "**[Epoch 45]************************************************\n",
      "\tTraining Loss: 0.00363 | Evaluation Loss: 0.00357\n",
      "**[Epoch 46]************************************************\n",
      "\tTraining Loss: 0.00361 | Evaluation Loss: 0.00354\n",
      "**[Epoch 47]************************************************\n",
      "\tTraining Loss: 0.00361 | Evaluation Loss: 0.00355\n",
      "**[Epoch 48]************************************************\n",
      "\tTraining Loss: 0.00360 | Evaluation Loss: 0.00352\n",
      "**[Epoch 49]************************************************\n",
      "\tTraining Loss: 0.00362 | Evaluation Loss: 0.00356\n",
      "**[Epoch 50]************************************************\n",
      "\tTraining Loss: 0.00361 | Evaluation Loss: 0.00353\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 12657<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/home/pablo/Desktop/ML/Pruebas/VAEPerceptualPlants/wandb/run-20210620_132539-m8iu1drm/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/home/pablo/Desktop/ML/Pruebas/VAEPerceptualPlants/wandb/run-20210620_132539-m8iu1drm/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>Loss (training)</td><td>0.00361</td></tr><tr><td>Loss (testing)</td><td>0.00353</td></tr><tr><td>_runtime</td><td>2636</td></tr><tr><td>_timestamp</td><td>1624219775</td></tr><tr><td>_step</td><td>49</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>Loss (training)</td><td>█▆▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>Loss (testing)</td><td>▇▆▄█▃▃▂▃▃▃▂▅▂▂▄▂▂▂▂▃▂▂▂▂▂▁▂▁▂▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">dainty-snowflake-3</strong>: <a href=\"https://wandb.ai/tecai/VAE/runs/m8iu1drm\" target=\"_blank\">https://wandb.ai/tecai/VAE/runs/m8iu1drm</a><br/>\n                "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Paths of data.\n",
    "dataPathTrain = '/home/pablo/Desktop/ML/Proyectos/Proyecto III/data/corrida1/noLabel'\n",
    "dataPathTest = '/home/pablo/Desktop/ML/Proyectos/Proyecto III/data/corrida1/labelTest'\n",
    "displayPath = './predictedImages'\n",
    "\n",
    "savePath = '/media/pablo/Disquito/IA/VAE/firstest'\n",
    "\n",
    "lossNetowrk = perceptual_net=VGG16(5) \n",
    "# Network.\n",
    "network = FourLayerCVAE(input_size=(config[inputSize], config[inputSize]), z_dimensions=1024, variational=True, gamma=0.001, perceptual_net=lossNetowrk)\n",
    "\n",
    "# Execute.\n",
    "executeTest(network, (dataPathTrain, dataPathTest), None, savePath, display=displayPath)"
   ]
  }
 ]
}